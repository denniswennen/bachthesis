#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass classicthesis
\use_default_options true
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding default
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine natbib_numerical
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
The mashup web service 
\begin_inset Note Note
status collapsed

\begin_layout Subsection*
Description of problems and open questions, How do I tackle this problem
 
\end_layout

\begin_layout Plain Layout
Middle chapters
\end_layout

\begin_layout Plain Layout
In some theses, the middle chapters are the journal articles of which the
 student was major author.
 There are several disadvantages to this format.
 One is that a thesis is both allowed and expected to have more detail than
 a journal article.
 For journal articles, one usually has to reduce the number of figures.
 In many cases, all of the interesting and relevant data can go in the thesis,
 and not just those which appeared in the journal.
 The degree of experimental detail is usually greater in a thesis.
 Relatively often a researcher requests a thesis in order to obtain more
 detail about how a study was performed.
\end_layout

\begin_layout Plain Layout
Another disadvantage is that your journal articles may have some common
 material in the introduction and the "Materials and Methods" sections.
\end_layout

\begin_layout Plain Layout
The exact structure in the middle chapters will vary among theses.
 In some theses, it is necessary to establish some theory, to describe the
 experimental techniques, then to report what was done on several different
 problems or different stages of the problem, and then finally to present
 a model or a new theory based on the new work.
 For such a thesis, the chapter headings might be: Theory, Materials and
 Methods, {first problem}, {second problem}, {third problem}, {proposed
 theory/model} and then the conclusion chapter.
 For other theses, it might be appropriate to discuss different techniques
 in different chapters, rather than to have a single Materials and Methods
 chapter.
\end_layout

\begin_layout Plain Layout
Here follow some comments on the elements Materials and Methods, Theory,
 Results and discussion which may or may not correspond to thesis chapters.
\end_layout

\begin_layout Plain Layout
Materials and Methods 
\end_layout

\begin_layout Plain Layout
This varies enormously from thesis to thesis, and may be absent in theoretical
 theses.
 It should be possible for a competent researcher to reproduce exactly what
 you have done by following your description.
 There is a good chance that this test will be applied: sometime after you
 have left, another researcher will want to do a similar experiment either
 with your gear, or on a new set-up in a foreign country.
 Please write for the benefit of that researcher.
 In some theses, particularly multi-disciplinary or developmental ones,
 there may be more than one such chapter.
 In this case, the different disciplines should be indicated in the chapter
 titles.
\end_layout

\begin_layout Plain Layout
Theory 
\end_layout

\begin_layout Plain Layout
When you are reporting theoretical work that is not original, you will usually
 need to include sufficient material to allow the reader to understand the
 arguments used and their physical bases.
 Sometimes you will be able to present the theory ab initio, but you should
 not reproduce two pages of algebra that the reader could find in a standard
 text.
 Do not include theory that you are not going to relate to the work you
 have done.
 When writing this section, concentrate at least as much on the physical
 arguments as on the equations.
 What do the equations mean? What are the important cases?
\end_layout

\begin_layout Plain Layout
When you are reporting your own theoretical work, you must include rather
 more detail, but you should consider moving lengthy derivations to appendices.
 Think too about the order and style of presentation: the order in which
 you did the work may not be the clearest presentation.
\end_layout

\begin_layout Plain Layout
Suspense is not necessary in reporting science: you should tell the reader
 where you are going before you start.
\end_layout

\begin_layout Plain Layout
Results and discussion 
\end_layout

\begin_layout Plain Layout
The results and discussion are very often combined in theses.
 This is sensible because of the length of a thesis: you may have several
 chapters of results and, if you wait till they are all presented before
 you begin discussion, the reader may have difficulty remembering what you
 are talking about.
 The division of Results and Discussion material into chapters is usually
 best done according to subject matter.
 Make sure that you have described the conditions which obtained for each
 set of results.
 What was held constant? What were the other relevant parameters? Make sure
 too that you have used appropriate statistical analyses.
 Where applicable, show measurement errors and standard errors on the graphs.
 Use appropriate statistical tests.
\end_layout

\begin_layout Plain Layout
Take care plotting graphs.
 The origin and intercepts are often important so, unless the ranges of
 your data make it impractical, the zeros of one or both scales should usually
 appear on the graph.
 You should show error bars on the data, unless the errors are very small.
 For single measurements, the bars should be your best estimate of the experimen
tal errors in each coordinate.
 For multiple measurements these should include the standard error in the
 data.
 The errors in different data are often different, so, where this is the
 case, regressions and fits should be weighted (i.e.
 they should minimize the sum of squares of the differences weighted inversely
 as the size of the errors.) (A common failing in many simple software packages
 that draw graphs and do regressions is that they do not treat errors adequately.
 UNSW student Mike Johnston has written a plotting routine[24] that plots
 data with error bars and performs weighted least square regressions.
 It is at http://www.phys.unsw.edu.au/3rdyearlab/graphing/graph.html).
 You can just 'paste' your data into the input and it generates a .ps file
 of the graph.
\end_layout

\begin_layout Plain Layout
In most cases, your results need discussion.
 What do they mean? How do they fit into the existing body of knowledge?
 Are they consistent with current theories? Do they give new insights? Do
 they suggest new theories or mechanisms?
\end_layout

\begin_layout Plain Layout
Try to distance yourself from your usual perspective and look at your work.
 Do not just ask yourself what it means in terms of the orthodoxy of your
 own research group, but also how other people in the field might see it.
 Does it have any implications that do not relate to the questions that
 you set out to answer?
\end_layout

\end_inset


\end_layout

\begin_layout Section
Mashing up content
\end_layout

\begin_layout Standard
Mashup is a new application development approach that allows users to aggregate
 multiple services to create a service for a new purpose.
 The term implies easy, fast integration, frequently using open APIs and
 data sources to produce enriched results that were not necessarily the
 original reason for producing the raw source data.
 The main characteristics of the mashup are combination, visualization,
 and aggregation.
 It is important to make existing data more useful, moreover for personal
 and professional use.
 To be able to permanently access the data of other services, mashups are
 generally client applications or hosted online.
 Since 2010, two major mashup vendors have added support for hosted deployment
 based on Cloud computing solutions; that are Internet-based computing,
 whereby shared resources, software, and information are provided to computers
 and other devices on demand, like the electricity grid.
 In the past years, more and more Web applications have published APIs that
 enable software developers to easily integrate data and functions instead
 of building them by themselves.
 Mashups can be considered to have an active role in the evolution of social
 software and Web 2.0.
 mashup composition tools are usually simple enough to be used by end-users.
 They generally do not require programming skills and rather support visual
 wiring of GUI widgets, services and components together.
 Therefore, these tools contribute to a new vision of the Web, where users
 are able to contribute.
 
\begin_inset CommandInset citation
LatexCommand citep
key "Wikipedia:mashup"

\end_inset


\end_layout

\begin_layout Standard
In contrast, even if the Mashup approach opens new and broader opportunities
 for data/service consumers, the development process still requires the
 users to know not only how to write code using programming languages, but
 also how to use the different Web API's from different services.
 The tools for creating mashups are supposed to target “non-expert” users,
 but a programming knowledge is usually required.
 Some tools require considerable programming effort since the whole process
 needs to be implemented manually using instructions expressed in programming
 language such as Java Script.
 Others necessitate medium programming effort given that only some functionaliti
es need to be coded in an explicit way using a programming language; a graphical
 interface is offered to the user to express most of operations.
 At this time, there is no tool that requires low or no programming effort
 by the user to build a Mashup, which is necessary to claim that the tools
 are targeted for end-users.
\begin_inset CommandInset citation
LatexCommand citep
key "DiLorenzo:2009:DIM:1558334.1558343"

\end_inset

 In order to solve this problem, there is increasing effort put into developing
 tools which are designed to support users with little programming knowledge
 in Mashup applications development.
 
\end_layout

\begin_layout Subsection
Mashup tools
\end_layout

\begin_layout Standard
Currently a number of Mashup tools exist.
 For this work some of the most know tools are explained to give a view
 on the current state of these tools and understand their general approach
 to data integration.
\begin_inset CommandInset citation
LatexCommand citep
key "DiLorenzo:2009:DIM:1558334.1558343"

\end_inset


\end_layout

\begin_layout Standard
Mashups using this approach can be termed as Rich Internet Applications
 (RIAs), meaning that they are very oriented towards the interactive user-experi
ence.
 Rich internet applications are one hallmark of what's now being termed
 "Web 2.0", the current generation of services available on the World Wide
 Web.
 A benefit of client-side mashing include less overhead on behalf of the
 mashup server; data can be retrieved directly from the content provider
 and a more seamless user-experience is created where pages can request
 updates for portions of their content without having to refresh the entire
 page.
\begin_inset CommandInset citation
LatexCommand citep
key "ibm:thenewbreed"

\end_inset


\end_layout

\begin_layout Subsubsection*
\begin_inset CommandInset label
LatexCommand label
name "sub:YahooPipes"

\end_inset

YahooPipes
\end_layout

\begin_layout Standard
Yahoo Pipes
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://pipes.yahoo.com
\end_layout

\end_inset

 is a web-based tool provided by Yahoo.
 that provides a graphical user interface for building data mashups that
 aggregate web feeds, web pages, and other services, creating Web-based
 apps from various sources, and publishing those apps.
 The application works by enabling users to "pipe" information from different
 sources.
 A pipe is composed of one or more modules, each one performing a single
 task like retrieving feeds from a web source, filter, sort or merge feeds..
 A typical example of a mashup is the 'New York Times through Flickr'
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://pipes.yahoo.com/pipes/pipe.info?_id=vvW1cD212xGMiR9aqu5lkA
\end_layout

\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "pipes:NYTtruFlickr"

\end_inset

, a pipe which takes The New York Times RSS feed and adds a photo from Flickr
 based on the keywords of each item.
 Also the 'Apartment Near Something'
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://pipes.yahoo.com/pipes/pipe.info?_id=1mrlkB232xGjJDdwXqIxGw
\end_layout

\end_inset

 made Yahoo Pipes more popular.
 
\end_layout

\begin_layout Standard
An API documentation with examples are provided.
 There is also a community with a forum to post questions and pipes created
 by other can be browsed and forked onto your own creations.
 That way work done by others can easily be duplicated for further use.
 Pipes can be cloned and adapted to differents needs for each user.
 The possibilities are endless, which enables a lot of creativity in collecting
 and creating content.
 It is a fairly easy application for aggreating data, with a visual User
 Interface.
 Users can create the pipe they want easily, via a visual drag and drop
 interface.
 This interface with a human-readable output makes it more accesible for
 the less skilled programmers.
 The users can build mashup applications by aggregating and manipulating
 data from web feeds (RSS, ATOM), web pages, and other services.
 (e.g.
 CSV) The content can be viewed on the pipe's website as a list of items
 or can be viewed on a Yahoo Map.
 Another option is to export the output to a webpage by embedding the 'Yaoo
 Badge' code or retrieving the JSON
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
JavaScrip Object Notation, further explained in the next chapter.
\end_layout

\end_inset

 or RSS output.
 The webservice implemented in this work, provides additional functionality
 by going on step further: making the data more accessable for visualisation.
 In JavaScript, there are written a lot of libraries to visualise data in
 all kinds of ways.
 But there is one mayor drawback and that is the access to the data.
 The libraries that make use of JavaScript, have limited or library-specific
 functionality in terms of data access.
\end_layout

\begin_layout Subsubsection*
Popfly (Deprecated, since August 24, 2009)
\end_layout

\begin_layout Standard
Popfly is a visual component-based Web-based Mashup application by Microsoft.
 It allows the users to create a Mashup combining data and media sources.
 Reusable components, or blocks, can act as middlemen between externally
 provisioned services, such as other Web services or implement a useful
 function in JavaScript.
 Blocks have operations with inputs and outputs, which are specified in
 a dedicated XML descriptor.
 Each block is associated to a service like for example Flicker and exposes
 one or more functionalities.
 A block might also act as a display surface — that is, a piece of user
 interface that takes data from other blocks and displays them, letting
 the user interact with them and enabling the mashup developer to lay out
 the mashup application.
\begin_inset CommandInset citation
LatexCommand citep
key "Yu:2008:UMD:1439188.1439257"

\end_inset


\end_layout

\begin_layout Standard
In fact Popfly is much more about data visualization than data manipulation,
 so the “mashed” data can be only visualized using the provided visualization
 tool.
 On July 16, 2009, the Popfly team announced that the Popfly service would
 be discontinued on August 24, 2009.
 All sites, references and resources of popfly were taken down, and this
 product is considered defunct.
\end_layout

\begin_layout Subsubsection
Google Mashup Editor (Deprecated, since January 14, 2009)
\end_layout

\begin_layout Standard
Google Mashup Editor (GME) was an AJAX development framework and a set of
 tools that enable developers to quickly and easily create simple web applicatio
ns and mashups with Google services like Google Maps and Google Base.
 It offered a set of standard modules that lets users encapsulate and lay
 out external data.
 For example, the list module represents an RSS or Atom feed as a list,
 whereas the item module represents a single item in a feed.
 Modules can fire predefined events, which other modules can capture and
 act on accordingly.
 Creating mashups involves developing user interface templates that contain
 a mixture of XML control tags and HTML/CSS layout elements with embedded
 JavaScript code.
 At runtime, GME fills the user interface templates and presents them as
 Web pages
\begin_inset CommandInset citation
LatexCommand citep
key "Yu:2008:UMD:1439188.1439257"

\end_inset


\end_layout

\begin_layout Subsubsection*
MashMaker
\end_layout

\begin_layout Standard
is a web-based tool by Intel for editing, querying and manipulating web
 data.
 MashMaker is different from the other tools in that it works directly on
 web pages.
 In fact, MashMaker allows users to create a mashup by browsing and combining
 different web pages.
 The final goal of this tool is to suggest to the user some enhancements
 (mashups or widgets), if available, for the visited web pages.
\end_layout

\begin_layout Standard
Mash Maker
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://software.intel.com/en-us/articles/intel-mash-maker-mashups-for-the-masses/#
Product Overview
\end_layout

\end_inset

 provides an environment for integrating data from annotated source Web
 pages based on a powerful, dedicated browser plug-in.
 Rather than taking input from structured data sources such as RSS or Atom,
 Mash Maker lets users annotate Web pages’ structure while browsing and
 use such annotations to scrap contents from annotated pages.
 Advanced users can leverage the integrated structure editor to input XPath
 expressions using FireBug’s DOM Inspector (a plug-in for the Firefox Web
 browser).
 Composing mashups with Mash Maker occurs via a copy-and-paste paradigm,
 based on two modes of merging contents: whole page merging, in which the
 user inserts one page’s content as a header into another page; and item-wise
 merging, in which the user combines contents from two pages at row level,
 based on additional user annotations.
 You can use the two techniques to merge more than two pages
\begin_inset CommandInset citation
LatexCommand citep
key "Yu:2008:UMD:1439188.1439257"

\end_inset

.
\end_layout

\begin_layout Subsubsection*
Firefox Ubiquity
\end_layout

\begin_layout Standard
Ubiquity was a Mozilla Labs experiment that was in development from 2008
 to 2009.
 Its purpose was to explore whether a radically different type of interface
 to the Web — a task-centric, natural-language-based command line — could
 help us get common Web tasks done faster.
 A collection of quick and easy natural-language-derived commands that act
 as mashups of web services, thus allowing users to get information and
 relate it to current and other webpages.
 It also allows Web users to create new commands without requiring much
 technical background.
 
\end_layout

\begin_layout Standard
Development is currently on indefinite hiatus.
 The experiment will most likely be revisited at some point in the future.
 In the meantime, the Ubiquity extension for Firefox is still available
 for download.
 Also, some of the ideas from Ubiquity are being implemented in Mozilla
 Firefox' Add-on Builder and SDK.
\end_layout

\begin_layout Subsubsection*
WSO2
\end_layout

\begin_layout Standard
The WSO2 Mashup Server is an open source mashup platform that hosts JavaScript
 based mashups.
 WSO2 is stated as the ideal platform for defining composite services for
 user interfaces and mobile applications, a simple way to deploy services
 developed in JavaScript.
 Access to REST and web services, feeds, and scraped web pages with data
 scripted together quickly using common Web developer skills, the result
 being a new service, or a web page, gadget, email or instant message.
 The ability to secure hosted Mashups, support for both recurring and longer-run
ning tasks and service lifecycles, monitoring and configuration of security
 and quality of service settings such as throttling.
 The source code is freely available under the open source Apache License.
 It provides a runtime platform for developing and deploying mashups.
 It can be downloaded and deployed locally or within an organization.
\end_layout

\begin_layout Subsection
Mashup levels
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
A lot of searching and analysing the problem.
 I spend a lot of time trying to figure out what exactly the problem is
 and how to define it.
 Also in this view, where I can bring the most value.
 What are the difficult parts, which ones are already investigated or need
 some more attention.
 It's not just one item to research, but a complex situation.
 This can have several views and approaches, each aimed at specific targets.
\end_layout

\end_inset

Data mediation involves converting, transforming, and combining the data
 elements from one or multiple services to meet the needs of the operations
 of another.
 For instance, mediating between data models of tags represented in both
 the Flickr and the Google Map API'.
 
\end_layout

\begin_layout Standard
Process (or protocol) mediation in essence is providing the right bridge
 between different services to create a new process.
 For instance, process mediation includes invoking the various service methods,
 waiting for asynchronous messages, and sending any necessary confirmation
 messages.
 User interface customization is used to elicit user information as well
 as to display intermittent and final process information to the user.
 Depending on the domain, the user interface customization can be as simple
 as an HTML page, a more complex series of input forms, or an interactive
 AJAX Use Interface.
 
\begin_inset CommandInset citation
LatexCommand citep
key "LanguageforWebAPIs"

\end_inset


\end_layout

\begin_layout Standard
Different types of mashups:
\end_layout

\begin_layout Itemize
A Consumer Mashup is aimed at the general public.
 Consumer mashups, when offered with web-based mashup facility can prove
 to be a very effective means for customer personalization of data/viewing,
 this is typically were the user’s use their web browser to combine and
 reformat the data according to their needs.
\end_layout

\begin_layout Itemize
Data Mashups combine similar types of media and information from multiple
 sources into a single representation.
 Yahoo Pipes is an example of a data mashup.
\end_layout

\begin_layout Itemize
Business Mashups focus data into a single presentation and allow for collaborati
ve action among businesses and developers.
 This works well for an agile development project, which requires collaboration
 between the developers and customer proxy for defining and implementing
 the business requirements.
 Business mashups differ from consumer mashups in the level of integration
 with business computing environments, security and access control features,
 governance, and the sophistication of the programming tools (mashup editors)
 used (typesofmashups:Peenikal).
\end_layout

\begin_layout Standard
Architecturally, there are two styles of mashups: web-based and server-based.
 Web-based mashups typically use the user’s web browser to combine and reformat
 the data, server-based mashups analyze and reformat the data on a remote
 server and transmit the data to the user’s browser in its final form.
\end_layout

\begin_layout Subsubsection
Architecture 
\end_layout

\begin_layout Standard
A mashup application is architecturally composed of three different components
 that are meant to operate seperatly on a logical and physical level (they
 are likely separated by both network and organizational boundaries).
 The components are: the Application Programmable Interface (API) or content
 providers, the mashup site with the User Interface, and the client's Web
 browser
\begin_inset CommandInset citation
LatexCommand citep
key "ibm:thenewbreed"

\end_inset

.
 
\end_layout

\begin_layout Itemize
The API/content providers are the (sometimes unwitting) providers of the
 content being mashed.
 In the ChicagoCrime.org mashup example, Google and the Chicago Police Department
 deliver the data.
 To facilitate data retrieval, providers often expose their content through
 Web-protocols such as REST, Web Services, and RSS/Atom.
 We will describe these technologies more in detail below in 
\begin_inset Flex CT - auto cross-reference
status collapsed

\begin_layout Plain Layout

sec:Technologies
\end_layout

\end_inset

.
 However, many interesting potential data-sources do not (yet) conveniently
 expose their API's.
 Mashups that extract content from sites like Wikipedia, TV Guide, and virtually
 all government and public domain Web sites do so by a technique known as
 screen scraping.
 In this context, screen scraping denotes the process by which a tool attempts
 to extract information from the content provider by attempting to parse
 the provider's Web pages, which were originally intended for human consumption.
 
\end_layout

\begin_layout Itemize
The mashup site.
 This is where the mashup is hosted.
 Interestingly enough, just because this is where the mashup logic resides,
 it is not necessarily where it is executed.
 On one hand, mashups can be implemented similarly to traditional Web applicatio
ns using server-side dynamic content generation technologies like Java servlets,
 CGI, PHP or ASP.
 Alternatively, mashed content can be generated directly within the client's
 browser through client-side scripting; via JavaScript or applets.
 This client-side logic is often the combination of code directly embedded
 in the mashup's Web pages as well as the API libraries or applets from
 other Web Services, which provide the code to interact with the functionality
 made available.
 Mashups using this approach can be termed rich internet applications (RIAs),
 meaning that they are very oriented towards the interactive user-experience.
 (Rich internet applications are one hallmark of what's now being termed
 "Web 2.0", the next generation of services available on the World Wide Web.)
 The benefits of client-side mashing include less overhead on behalf of
 the mashup server (data can be retrieved directly from the content provider)
 and a more seamless user-experience (pages can request updates for portions
 of their content without having to refresh the entire page).
 The Google Maps API is intended for access through browser-side JavaScript,
 and is an example of client-side technology.
 Often mashups use a combination of both server and client-side logic to
 achieve their data aggregation.
 Many mashup applications use data that is supplied directly to them by
 their user base, making (at least) one of the data sets local.
 Additionally, performing complex queries on multiple-sourced data requires
 computation that would be infeasible to perform within the client's Web
 browser.
 
\end_layout

\begin_layout Itemize
The client's Web browser.
 This is where the application is rendered graphically and where user interacts
 with the application logic.
 As described above, mashups applications often use client-side logic to
 assemble and compose the mashed content.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Here in this section I provide more detail on my approach.
 There is a difference between the literature study and the implementation.
 The first is a general synthesis, an idea or a problem which requires some
 thinking work to come up with a solution.
 The second part is, as it we re, a more lined selection of the problem
 which than can be tackeld.
 Description of problems and open questions, How do I tackle this problem
 Middle chapters A lot of searching and analysing the problem.
 I spend a lot of time trying to figure out what exactly the problem is
 and how to define it.
 Also in this view, Where I can bring the most value? What are the difficult
 parts? Which ones are already investigated or need some more attention?
 
\end_layout

\end_inset


\end_layout

\begin_layout Section
Technologies
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Technologies"

\end_inset

This section gives an overview of the common technologies that are used
 the development of mashups.
 Any further information or specification can be found by following the
 links associated with the protocols.
 
\end_layout

\begin_layout Subsection*
Ajax 
\end_layout

\begin_layout Standard
There is some dispute over whether the term Ajax is an acronym or not (some
 would have it represent "Asynchronous JavaScript + XML").
 Regardless, Ajax is considered as a Web application model rather than a
 specific technology.
 It comprises several technologies focused around the asynchronous loading
 and presentation of content: XHTML and CSS for style presentation, the
 Document Object Model (DOM) API exposed by the browser for dynamic display
 and interaction With Ajax, asynchronous data exchange, typically of XML
 data and browser-side scripting, where mostly JavaScript is used.
 Web applications and can send data to, and retrieve data from, a server
 without interfering with the display and behavior of the existing page.
 When used together, the goal of these technologies is to create a, interactive
 and smooth Web experience for the user when interacting with the website
 elements, by exchanging small amounts of data with the content servers
 rather than reload an entire webpage.
 You can construct Ajax engines for mashups from various Ajax toolkits and
 libraries, usually implemented in JavaScript.
 The Google Maps API includes a proprietary Ajax engine, and the effect
 it has on the user experience is powerful: it behaves like a truly local
 application in that there are no scrollbars to manipulate or translation
 arrows that force page reloads.
 
\end_layout

\begin_layout Subsection*
Web protocols: SOAP and REST 
\end_layout

\begin_layout Standard
Both SOAP (Services-Oriented Access Protocol)
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://www.w3.org/TR/soap12-part1/#intro
\end_layout

\end_inset

 and REST (Representational State Transfer)
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://en.wikipedia.org/wiki/Representational_State_Transfer
\end_layout

\end_inset

 are platform independent protocols for communicating with remote services.
 As part of the service-oriented architecture paradigm, clients can use
 SOAP and REST to interact with remote services without knowledge of their
 underlying platform implementation: the functionality of a service is completel
y conveyed by the description of the messages that it requests and responds
 with.
 
\end_layout

\begin_layout Standard
SOAP is a fundamental technology of the Web Services paradigm.
\begin_inset CommandInset citation
LatexCommand citep
key "ibm:thenewbreed"

\end_inset

 Because its focus has shifted from object-based systems towards the interoperab
ility of message exchang, the protocol name changes.
 Originally the term was an acronym for Simple Object Access Protocol, but
 it has been renamed to Services-Oriented Access Protocol (or just SOAP).
 There are two main components of the SOAP specification: The first is the
 use of an platform independent XML message format for encoding, and the
 second is the message structure, which consists of a header and a body.
 The header is used to exchange contextual information that is not specific
 to the application payload (the body), such as authentication information.
 The SOAP message body encapsulates the application-specific payload.
 SOAP API's for Web services are described by WSDL documents.
 WSDL is an XML format for describing network services as a set of endpoints
 operating on messages containing either document-oriented or procedure-oriented
 information.
 SOAP messages are typically transfered over HTTP transport, although other
 transports (such as JMS or e-mail) can also be used.
 
\end_layout

\begin_layout Standard
REST is a technique of Web-based communication using just HTTP and XML.
 It is an approach for getting information content from a Web site by reading
 a designated Web page that contains an XML (Extensible Markup Language)
 file that describes and includes the desired content.
 Unlike the typical verb-based interfaces that you find in modern programming
 languages (which are composed of diverse methods such as get(), set(),
 etc..), REST fundamentally supports only a few operations (that is POST,
 GET, PUT, DELETE) that are applicable to all pieces of information.
 The emphasis in REST is on the pieces of information themselves, called
 resources.
 For example, a resource record for an employee is identified by a URI,
 retrieved through a GET operation and updated by a PUT operation, and so
 on.
 The Yahoo Maps Geocoding service is a REST-based service.
 To call it, an appropriate URL is specified, along with any parameter-value
 pairs.
\end_layout

\begin_layout Subsection*
\begin_inset CommandInset label
LatexCommand label
name "sub:Screen-scraping"

\end_inset

Screen scraping 
\end_layout

\begin_layout Standard
As mentioned earlier, lack of API's from content providers often force mashup
 developers to use other, less elegant tools to retrieve the information
 for the mashup.
 Scraping is the process of using software tools to parse and analyze content
 that was originally written to be human readable in order to extract semantic
 data structures representative of that information that can be used and
 manipulated programmatically.
 A handful of mashups use screen scraping technology for gathering data,
 especially when pulling data from the public sectors or sources that lack
 the support of an API.
 For example, in the implemented webservice this tool is used to retrieve
 information on the input parameters, as the Yahoo Pipes Web Service did
 not include any API for acquiring that data.
 Screen scraping is often considered an inelegant solution, and for good
 reasons.
 It has two primary inherent drawbacks.
 The first is that, unlike APIs with interfaces, scraping has no specific
 programmatic contract between content-provider and content-consumer.
 Scrapers must design their tools around a model of the source content and
 hope that the provider consistently adheres to this model of presentation.
 Web sites have a tendency to overhaul their look-and-feel periodically
 to remain up to date and in style, which brings severe maintenance headaches
 on behalf of the scrapers because their tools are likely to fail.
 The second issue is the lack of a decent toolkit to reuse screen-scraping
 software; also known as scrAPIs.
 The lack of such APIs and toolkits is largely due to the application-specific
 needs of each individual scraping tool.
 This leads to large development overheads as designers are forced to reverse-en
gineer content, develop data models, parse, and aggregate raw data from
 the provider's site.
 
\end_layout

\begin_layout Subsection*
Semantic Web and RDF 
\end_layout

\begin_layout Standard
The inelegant aspects of screen scraping are directly traceable to the fact
 that content created for human consumption does not make good content for
 automated machine consumption.
 The Semantic Web is a mesh of information linked up in such a way as to
 be easily processable by machines, on a global scale.
 In the context of the Semantic Web, the term information is different from
 data; data becomes information when it becomes understandable and has a
 meaning.
 It has the goal of creating Web infrastructure that merges data with metadata
 to give it meaning and so making it suitable for automation, integration
 and re-use.
 The W3C family of specifications collectively known as the Resource Description
 Framework (RDF) serves this purpose of providing methodologies to establish
 syntactic structures that describe data.
 An RDF-Schema adds to RDF's ability to encode concepts in a machine-readable
 way, in extend because XML in itself can describe the same piece of data
 in too many ways so it's meaning is to ambiguous.
 Once data objects can be described in a data model, RDF provides for the
 construction of relationships between data objects through subject-predicate-ob
ject triples; e.g.
 'subject S has relationship R with object O'.
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
The combination of data model and graph of relationships allows for the
 creation of ontologies, which are hierarchical structures of knowledge
 that can be searched and formally reasoned about.
 For example, you might define a model in which a "carnivore-type" as a
 subclass of "animal-type" with the constraint that it "eats" other "animal-type
", and create two instances of it: one populated with data concerning cheetahs
 and polar bears and their habitats, another concerning gazelles and penguins
 and their respective habitats.
\end_layout

\end_inset

 Inference engines might then "mash" these separate model instances and
 give it a meaning.
 RDF data is quickly finding adoption in a variety of domains such RSS or
 more social networking applications like FOAF (Friend of a friend).
\end_layout

\begin_layout Standard
In addition, RDF software technology and components are beginning to reach
 a level of maturity, especially in the areas of RDF query languages (such
 as RDQL and SPARQL) and programmatic frameworks and inference engines (such
 as Jena and Redland)
\begin_inset CommandInset citation
LatexCommand citep
key "ibm:thenewbreed"

\end_inset

.
 
\end_layout

\begin_layout Subsection*
RSS and ATOM 
\end_layout

\begin_layout Standard
RSS (originally RDF Site Summary, often dubbed Really Simple Syndication)
 is a family of XML-based formats.
 In this context, syndication implies that a Web site that wants to distribute
 content creates an RSS document and registers the document with an RSS
 publisher.
 An RSS-enabled client can then check the publisher's feed for new content
 and react to it in an appropriate manner.
 RSS can be used in different ways for content distribution, its most widespread
 usage is in distributing news headlines on the Web.
 RSS has been adopted to syndicate a wide variety of content, ranging from
 news articles and headlines, changelogs for CVS checkins or Wiki pages,
 project updates, and even audiovisual data such as radio programs.
 Version 1.0 is RDF-based, but the most recent, version 2.0, is not.
 Atom is a newer, but similar, syndication protocol.
 It is a proposed standard at the Internet Engineering Task Force (IETF).
 The name Atom applies to a pair of related standards.
 The Atom Syndication Format is an XML language used for web feeds, while
 the Atom Publishing Protocol (AtomPub or APP) is a simple HTTP-based protocol
 for creating and updating web resources.
 The standard seeks to provide better metadata than RSS, maintain better
 and more rigorous documentation and show a beginning efort in the creation
 of a common data representation.
 These syndication technologies are great for mashups that aggregate event-based
 or update-driven content, such as news and weblog aggregators.
\end_layout

\begin_layout Section
Data integration challenges
\end_layout

\begin_layout Standard
There are a number of challenges to address when integrating data from different
 parties.
 One of the important characteristics of the Web and also one of the most
 interesting challenges is certainly its heterogeneity.
 This heterogeneity can be seen on data, processes, and even user interfaces.
 We classify these challenges into four groups: text/data mismatch, object
 identifiers and schema mismatch, abstraction level mismatch, data accuracy.
\end_layout

\begin_layout Subsection*
Text/Data Mismatch 
\end_layout

\begin_layout Standard
Heterogeneity can occurs not only in the schema of the data, but also in
 the actual data values themselves.
 Human language is often ambiguous - the same company might be referred
 to in several variations (e.g.
 IBM, International Business Machines, and Big Blue).
 The ambiguity makes cross-linking with structured data difficult.
 To fully integrate data from multiple sources one needs to handle both
 the semantic-level heterogeneity and the data-level heterogeneity.
\end_layout

\begin_layout Standard
Object Identity and Separate Schema Structured data is available in a lot
 of of formats.
 Lifting the data to a common data format is thus the first step.
 But even if all data is available in a common format, in practice sources
 differ in how they state what essentially the same fact is.
 The differences exist both on the level of individual objects and the schema
 level and in addition, each source typically uses its own schema.
 Thus, Methods have to be in place for reconciling different representations
 of objects and schema.
 
\end_layout

\begin_layout Subsection*
Abstraction Levels 
\end_layout

\begin_layout Standard
Since data can being published at different levels of abstraction (e.g.
 person, company, country, or sector), data aggregated for the individual
 viewpoint may not match data e.g.
 from statistical offices.
 There are differences in geographic aggregation (e.g.
 region data from one source and country-level data from another).
 A related issue is the use of local currencies (USD vs.
 EUR) which have to be reconciled in order to make data from different sources
 comparable and understandable for analysis.
\end_layout

\begin_layout Standard
Another problem is the fact that much more of the data we need to manage
 is semi-structured, and is often the result of trying to extract structure
 from unstructured data.
 Hence, we need to manage data where the values, attributes names and semantics
 are often uncertain or ambiguous.
\end_layout

\begin_layout Subsection*
Data Quality 
\end_layout

\begin_layout Standard
Data quality is a general challenge when automatically integrating data
 from autonomous sources.
 In an open environment the data aggregator has little to no influence on
 the data publisher.
 Data is often not error free, and combining data often makes the problem
 worse.
 Especially when performing reasoning (automatically inferring new data
 from existing data), erroneous data has potentially devastating impact
 on the overall quality of the resulting dataset.
 
\end_layout

\begin_layout Standard
Hence, a challenge is how data publishers can coordinate in order to fix
 problems in the data or blacklist sites which do not provide reliable data.
 Methods and techniques are needed to; check integrity, accuracy, highlight,
 identify and sanity check, corroborating evidence; asses the probability
 that a given statement is true, equate weight differences between market
 sectors or companies; act as clearing houses for raising and settling disputes
 between competing (and possibly conflicting) data providers and interact
 with messy erroneous web data of potentially dubious provenance and quality.
 In summary, errors in signage, amounts, labelling, and classification can
 seriously impede the utility of systems operating over such data
\begin_inset CommandInset citation
LatexCommand citep
key "dataintegrationchallenge"

\end_inset

.
\end_layout

\begin_layout Section
Data handling
\end_layout

\begin_layout Standard
When database schemas for the same domain are created by independent parties,
 they will almost always be quite different from each other.
 These differences are referred to as semantic heterogeneity.
 This phenomenon also appears in the presence of multiple XML documents,
 JSON objects, web services and ontologies or more broadly, whenever there
 is more than one way to structure a data set.
 In this section, we will provide some insights in the topics that arised
 when working with data from different sources.
\end_layout

\begin_layout Subsection
Format and access
\end_layout

\begin_layout Standard
A Mashup application integrates data described in different formats, coming
 from different sources.
 For example, a web feed format is used to publish updated content such
 as blog entries, frequently updated news and so on; tabular format is suitable
 for describing table-based data models such as spreadsheets, structered
 text and CSV files.
 HTML and XML, markup-based format  is commonly used to publish content
 rich multimedia data such as video, audio and images.
 These types of data can be available to the user from different data sources,
 where the most common sources can be a traditional database systems, local
 files that are available in the user's file system, Web pages, Web services
 and Web applications.
\end_layout

\begin_layout Standard
Providers often expose their content and interface through web API's to
 make Web data retrieval more easy.
 An Aplication Programming Interface (API) is a particular set of specifications
 that software programs can follow to communicate with each other.
 It serves as an interface between different software programs and facilitates
 their interaction, similar to the way the user interface facilitates interactio
n between humans and computers.
 API's can be also seen as a useful mediation for data and applications.
 By allowing data retrieval and data exchange between applications, API's
 increase the integration between the web services.
 On the Web, providers like Google, Yahoo, eBay and Flickr offer web APIs
 for retrieving content from their web sites.
 However, content for some common data sources is protected and not always
 exposed trought the API's.
 Other techniques are needed to extract information; a tool for this purpose
 is known as screen scraping.
 API's help the developers access and consume data and resources without
 going in to much detail on their internal structure.
 Here we consider the role of APIs from the data integration point of view
 in the sense that they offer specific types and formats of data.
 It should be noted that an API can offer several formats of data, e.g., CSV,
 XML, JSON etc.
 and depends on the functionality of the API.
\end_layout

\begin_layout Subsection
Internal data model
\end_layout

\begin_layout Standard
As stated before, the objective of a Mashup application is to combine different
 resources, data in our case, to produce a new application.
 These resources come generally from different sources, are in different
 formats with different semantics.
 To support this, each mashup tool uses an internal data model.
 An internal data model is a single global schema that represents a general
 view on the data.
 A Mashup tool’s internal data model can be either (i) graphbased or (ii)
 object-based.
 In a graph-based model, the graph refers to the model based on a XML specificat
ion.
 This can include pure XML, RDF, RSS, JSON etc.
 Most of the Mashup applications use a Graph-based model as an internal
 data model.
 The use is motivated by the fact that most of today’s data available on
 the web are in this format and also, most of the Mashup tools are available
 via the Web.
 That is, all the data that are used by the Mashup tools, in this category,
 transform the input data into an XML representation before processing it.
 In an Objectbased model, the internal data is in the form of objects (in
 the classical sense of the object-oriented programming).
 An object is an instance of a class which defines the features of an element,
 including the element’s characteristics (its attributes, fields or properties)
 and the element’s behaviors (methods).
 It should be noted that in this case, there is no explicit transformation,
 performed by the tool, like in the case of the graph-based model, but the
 programmer needs to define the structure of the object according to her
 data
\begin_inset CommandInset citation
LatexCommand citep
key "DiLorenzo:2009:DIM:1558334.1558343"

\end_inset

.
\end_layout

\begin_layout Standard
Another data format that is gaining more popularity is JSON (which can be
 thought of a subset of JavaScript).
 JSON will be explained more in detail in sub:JSONand The format is a lot
 leaner than XML.
 This has several side-effects, e.g.
 JSON is generally smaller than a XML document and there for faster to work
 with.
 It can be parsed more efficiently because it can be parsed as JavaScript,
 which the built-in eval() function will do for you.
 It's considered unsafe to allow remote JavaScript to execute in this manner,
 but for the most part it's really perfectly fine.
 A big difference between JSON and XML (or HTML to be specific) is that
 HTML represents static content while JSON in conjunction with JavaScript
 can achieve dynamic content.
 It can be seen as a domain specific data source for browsers due to the
 nature of the relation between JSON and JavaScript.
\end_layout

\begin_layout Subsection
Data mapping
\end_layout

\begin_layout Standard
To instantiate an internal data model from an external data source, the
 Mashup tools must provide strategies to specify the correspondences between
 their internal data model and the desired data sources.
 This is achieved by means of data mapping.
 Data mapping is the process needed to identify the similarities and differences
 between the elements of the source data model and the internal data model.
 Generally speaking, a data mapping can be: (i) manual, where all the correspond
ences between the internal data model and the source data model are manually
 specified, one by one, by the application designer.
 In this case, the tool should then provide some facilities for the user
 to design the transformation.
 (ii) Semiautomatic, where the system exploits some meta-data (e.g., fields
 names and types) to propose some possible mapping configurations.
 Automatic, where all the correspondences between the two data models are
 automatically generated, without user intervention.
 This is a challenging issue in the data integration area.
 Since the Mashup area is in its “early stage
\begin_inset Quotes erd
\end_inset

, this type of mapping is not supported by any Mashup tool.
 It should be noted that the mapping process may require an intermediary
 step, i.e., a wrapping step, in order to transform the source format to the
 internal format, e.g., from CSV to XML.
\begin_inset CommandInset citation
LatexCommand citep
key "DiLorenzo:2009:DIM:1558334.1558343"

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
The JSON model provides a decent help in the process of data mapping.
\end_layout

\end_inset

 
\end_layout

\begin_layout Subsection
Data Flow Operators
\end_layout

\begin_layout Standard
Data flow operators allow performing operations either on the structure
 of the data (similar to the data definition language/ operators in the
 relational model), or on the data (content) itself (similar to the data
 manipulation language/ operator in the relational model).
 More concretely, data flow operators support: (i) restructuring of the
 schema of the incoming data, e.g., adding new elements, adding new attributes
 to elements; (ii) elaborating on a data set such as extracting a particular
 piece of information, combining specific elements that meet a given condition,
 change the value of some elements; (iii) building a new data set from other
 data sets such as merging, joining or aggregating data (similar to the
 concept of views in databases).
\end_layout

\begin_layout Subsection
Data Refresh
\end_layout

\begin_layout Standard
In some cases, for example the stock market, live data is generated and
 updated continuously.
 Various strategic decisions, especially in business practices, are generally
 taken last minute where the latest state and value of the data is the most
 meaningful.
 It is then important that a system propagates the updates of the data sources
 to the concerned user.
 An important issue in the dissemination of time-varying web data such as
 live data is the maintenance of temporal coherency.
\end_layout

\begin_layout Standard
There are two strategies dealing with the status of the data in the source,
 depending on the objective of the user: (i) pull strategy and (ii) push
 strategy.
\begin_inset CommandInset citation
LatexCommand citep
key "pushpull2002"

\end_inset

 The pull strategy is based on frequent and repetitive requests from the
 client.
 In the case of servers adhering to the HTTP protocol, clients need to frequentl
y pull the data based on the dynamics of the data and a user’s coherency
 requirements.
 The pulling frequency is set to be lower than the average update frequency
 of the data in the source itself.
 The freshness of the data depends on the pulling frequency, i.e., the higher
 the pulling frequency, the fresher the data and vice-versa.
 One of the main disadvantages of a high refresh frequency is that unnecessary
 requests may be generated to the server.
 In contrast in the push strategy, the client does not send requests but
 needs to register to the server.
 The servers that possess push capability maintain state information pertaining
 to clients and push only those changes that are of interest to a user.
 These two canonical techniques have complementary properties with respect
 to the level of temporal coherency maintained, communication overheads,
 state space overheads, and loss of coherency due to (server) failures.
\end_layout

\begin_layout Subsection
Mashup Output
\end_layout

\begin_layout Standard
A user might be interested in exporting the data flow result in another
 format in order to reuse it or to process it with another particular applicatio
n for further processing instead of visualizing it in the Mashup itself.
 That is, we can distinguish two main output categories: Human oriented
 output and application oriented output.
 In the Human oriented output, the output is targeted for human interpretation
 and to be readable, e.g.
 a visualization on a map, on an HTML page, etc.
 That is, for this category the output can be considered as the final stage
 of the whole process.
 For the processing oriented output, the output is mainly targeted for machine/m
ashup processing.
 This is interesting in the case where the considered data needs to be further
 processed.
 It should be noted that this category can, at some stage, include the first
 category, e.g., an RSS output can be at the same time visualized on an HTML
 page and also can be used by other applications for other processing tasks.
 
\end_layout

\begin_layout Standard
The Yahoo Pipes Web service provides a lot of functionality on data output.
 On their website a human oriented output is the default output; user created
 pipes can be browsed and excuted, where the result can be viewed on a HTML
 page or Yahoo Map.
 This view can even be exported into an own website.
 A Yahoo! Pipes badge allows you to have Pipes generated content by inserting
 the script code on your blog, website or social network.
 Currently there are 3 badges: a list, image and map badge.
 All badges can be configured by color, size, and number of items.
 Several processing oriented outputs are integrated into the API to export
 the data: RSS, JSON and PHP provide a more transparant access to web developers
 to use the data for further processing in their own web applications.
\end_layout

\begin_layout Subsection
Sharing
\end_layout

\begin_layout Standard
Mashups are based on the emerging technologies of the Web 2.0 in which people
 can create, annotate, and share information in an easy way.
 Enabling security and privacy for information sharing in this huge network
 is certainly a big challenge.
 This task is made more difficult especially since the targeted public with
 the Web 2.0 is, or supposed to be, a general public and not expert in computing
 or security.
 This dimension defines the modality that the tool offers to enable resources
 sharing by guaranteeing privacy and security in the created Mashup applications.
 This is a challenging area in the current Mashup and a lot of work remains
 to be done.
 This dimension includes the following three indicators: 1) What is shared
 in the Mashup?, 2) How is this shared? and 3) Who are the users with whom
 the shared resources are shared with? For the 'what', the shared resource
 can be total, partial, or nothing.
 The shared resource can be given different rights such as read only (user
 can read all entries but cannot write any entry), read/write (user can
 read and write all entries in the data), no access (user cannot read or
 write any entries).
 The 'who' as for it can be public a certain group, or a particular user.
 Notice that for each member, different sharing policies (what and how)
 can be specified and applied.
 For example, GME and Yahoo Pipes allow implementing sharing policies.
 In Yahoo Pipes, if a private element is used (Private string or Private
 text input) the code of the shared Mashup is available as well as the Mashup
 output.
 Also it is possible to share the created public or keep them private.
\begin_inset CommandInset citation
LatexCommand citep
key "DiLorenzo:2009:DIM:1558334.1558343"

\end_inset


\end_layout

\begin_layout Subsection
Data Security
\end_layout

\begin_layout Standard
Because a client-side mashup brings in code or content from another site,
 you need to assess the risks of these outside additions to the own website.
 In some cases, the risk is small.
 For example, if an image or an RSS feed is fetched, the image or content
 might not be available, which is a limited risk.
 The browser might display a symbol indicating that an image is missing,
 or the RSS feed might not appear on the page.
 But the application would not likely be impacted in any other way.
 In other words, the missing content would not damage the mashup service
 or the client browser.
 But when a JavaScript file from another site was included, the risk increases.
 In fact, bringing in a JavaScript file gets around a basic security protection
 for Ajax interactions called the browser security sandbox, also known as
 the XMLHttpRequest sandbox.
 Many mashups make use of the Ajax functionality.
 An XMLHttpRequest is a JavaScript object that is used to exchange data
 asynchronously between a web client and web server in an Ajax transaction.
 To protect against possible maliciousness attacks, most browsers allow
 JavaScript code that contains an XMLHttpRequest to communicate only with
 the site from which the browser loaded the code.
 The specification is known as the Same Orgin Policy..
\end_layout

\begin_layout Subsubsection*
Same origin policy
\end_layout

\begin_layout Standard
In creating mashups, a primary reason for using the proxy style is to contend
 with the basic security protection that the browser security sandbox provides.
 In a proxy-style mashup, a server-side proxy accesses the service.
 Because of that, a server-side mashup is not subject to the browser security
 sandbox and can connect to a site other than the server of origin to access
 a service.
 A client-side mashup avoids the constraints of the browser security sandbox
 because the service call is made from a dynamically created <script> tag,
 which can communicate with any domain to circumvent the Same origin policy.
 
\begin_inset CommandInset citation
LatexCommand citep
key "sunclientsidemashups:2007"

\end_inset


\end_layout

\begin_layout Standard
In computing, the same origin policy is an important security concept for
 a number of browser-side programming languages, such as JavaScript.
 The policy permits scripts running on pages originating from the same site
 to access each other's methods and properties with no specific restrictions,
 but prevents access to most methods and properties across pages on different
 sites.
\end_layout

\end_body
\end_document
